{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "id": "zcvTfvXOBpeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "import streamlit as st\n",
        "from vertexai.preview import generative_models\n",
        "from vertexai.preview.generative_models import GenerativeModel, Part, Content, ChatSession"
      ],
      "metadata": {
        "id": "3I4DI4i6pcyj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project = \"sample-gemini\"\n",
        "vertexai.init(project=project)\n",
        "\n",
        "config = generative_models.GenerationConfig(\n",
        "    temperature=0.4\n",
        ")\n",
        "model = GenerativeModel(\n",
        "    \"gemini-pro\",\n",
        "    generation_config=config\n",
        ")\n",
        "chat = model.start_chat()"
      ],
      "metadata": {
        "id": "YeeYo82OnycL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from vertexai.preview.generative_models import GenerativeModel, GenerationConfig\n",
        "\n",
        "# Initialize the session state with an empty messages list\n",
        "if 'messages' not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Define the llm_function for sending messages to the chat interface\n",
        "def llm_function(chat, message):\n",
        "    # This function sends the message to the chat interface\n",
        "    st.session_state.messages.append(message)\n",
        "\n",
        "# Initialize the GenerativeModel for chat\n",
        "model = GenerativeModel(\"gemini-pro\")\n",
        "\n",
        "# Implement Logic for Initial Prompt\n",
        "if len(st.session_state.messages) == 0:\n",
        "    initial_prompt = \"Introduce yourself as ReX, an assistant powered by Google Gemini. You use emojis to be interactive\"\n",
        "    llm_function(chat, initial_prompt)\n",
        "\n",
        "# Chat interface\n",
        "chat = st.empty()  # Create an empty slot for the chat interface\n",
        "\n",
        "# Streamlit app code\n",
        "st.title(\"Welcome to ReX, Your Assistant\")\n",
        "user_name = st.text_input(\"Please enter your name\")\n",
        "if user_name:\n",
        "    personalized_greeting = f\"Hello, {user_name}! I'm ReX, your assistant powered by Google Gemini. ðŸ˜Š\"\n",
        "    llm_function(chat, personalized_greeting)\n",
        "\n",
        "# Example: Displaying personalized greetings based on user input\n",
        "if len(st.session_state.messages) > 1:\n",
        "    last_message = st.session_state.messages[-1]\n",
        "    if \"name\" in last_message.lower():\n",
        "        llm_function(chat, \"Nice to meet you, I'm ReX! How can I assist you today?\")\n",
        "\n",
        "# End of Streamlit app code"
      ],
      "metadata": {
        "id": "whdQBrO8BBlP"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}